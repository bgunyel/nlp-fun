[module]
name = 'machine_translation'  # sentiment | tokenizer | machine_translation

# SENTIMENT ANALYSIS
[sentiment.train_config]
n_classes = 3
n_epochs = 10
batch_size = 32
mini_batch_size = 8

[sentiment.optimizer_config]
name = 'AdamW'
lr = 3e-4
weight_decay = 0.1
betas = [0.9, 0.95]
eps = 1e-8

[sentiment.model_config]
backbone = 'prajjwal1/bert-mini'
n_backbone_params_to_train = 10  # Last backbone params that are NOT frozen
fc_hidden_size = 256
dropout_prob = 0.25

# TOKENIZER TRAINING
[tokenizer.train_config]
dataset_name = 'tatoeba'
language = 'tur'
vocabulary_size = 11000

[tokenizer.optimizer_config]
name = 'Huggingface'

[tokenizer.model_config]
name = 'BPE'
pre_tokenizer = 'BertPreTokenizer'  # Whitespace | BertPreTokenizer

# MACHINE TRANSLATION
[machine_translation.train_config]
source_language = 'epo'
target_language = 'tur'
n_epochs = 2
batch_size = 4

[machine_translation.optimizer_config]
name = 'AdamW'
lr = 3e-4
weight_decay = 0.1
betas = [0.9, 0.95]
eps = 1e-8

[machine_translation.model_config]
name = 'MT'
bos_token = '<BOS>'
eos_token = '<EOS>'
max_sequence_length = 512

[machine_translation.model_config.encoder]
embedding_size = 256
hidden_size = 256
num_layers = 2
dropout_prob = 0.25

[machine_translation.model_config.decoder]
embedding_size = 256
hidden_size = 256
num_layers = 2
dropout_prob = 0.25